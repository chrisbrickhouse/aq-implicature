---
title             : "The title"
shorttitle        : "Title"

author: 
  - name          : "First Author"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "my@email.com"
  - name          : "Second Author"
    affiliation   : "1"
  - name          : "Third Author"
    affiliation   : "1"
    
affiliation:
  - id            : "1"
    institution   : "Stanford University"

author_note: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

abstract: |
  Enter abstract here. Each new line herein must be indented, like this line.
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["r-references.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes
mask              : no

class             : "man"
output            : papaja::apa6_pdf
---

```{r load_packages, include = FALSE}
library(papaja)
library(tidyverse)
library(magrittr)
library(readr)
library(png)
library(lme4)
library(ggthemes)
library(forcats)
library(brms)
```

```{r analysis_preferences}
# Seed for random number generation
set.seed(42)
```

# Introduction

* In a truth-value judgment task, how do we know whether an interpretation is literal or the result of an implicature computation?

Traditional Linking Hypotheses:
* If an implicature is calculated, the participant chooses a Non-True/Right response
* If an implicature is calculated, the participant chooses the Wrong/False response
* If an implicature is calculated, the participant chooses the lower end of the scale (2: wrong/False, 3: wrong, 4: wrong/kinda-wrong, 5: wrong/kinda-wrong)

Questions:
* Do these linking hypotheses give us different measures of implicature computation?
* If they do differ, which one is most stable?

Alternative Linking Hypothesis:
* RSA: Response behavior across conditions (utterance-card combinations) and dependent measures can be predicted by a linking hypothesis that assumes that participants are behaving like soft-optimal RSA speakers and provide a particular response (eg TRUE) to an utterance u if the RSA speaker probability of u (given the card) is within a particular probability interval (eg, within the interval [theta, 1]).

* Differences between traditional approaches and RSA: 1. The traditional linking hypotheses are based on a binary implicature/literal theory of pragmatic reasoning but RSA gives a continuous measure of pragmatic reasoning and allows for better predicting response behavior with multiple options.

# Background

* discussing the ways people in tha past have measured the "implicature rate".
* it seems like the literature takes the n(not-True)/n(Total) as the proporition of responses caused by implicature calculation
* BUT, I remember that Jesse Snedeker said it's NOT n(not-True)/n(Total) but it is n(False)/n(Total)
* However, this is probably not a consensus in the field because Katsos & Bishop consider the mid-point response "big" on the scale small-big-huge (strawberry) to be the result of implicture caculation

* what is the most common measure of "implicature rate" in the literature?
Binary True/False: Noveck 2001, 
Ternary: Katsos & Bishop 2011

# Methods

```{r importData, warning=FALSE, echo=FALSE}
data <- read_csv("experiments/pilot/3_processed_data/guessingGame_data.csv")

# define an implicature column for the ad-hoc and implicature trials
pragmatic_trials <- 
  data %>%
  filter(trial_type == "XY_XorY" | trial_type=="XY_X")

# adding a column that defines pragmatic vs. literal
pragmatic_trials$implicature<-1
pragmatic_trials[pragmatic_trials$response=="right",]$implicature <-0

# chaing trial type names to adhoc vs. scalar
pragmatic_trials$trial_type <-fct_recode(pragmatic_trials$trial_type, 
                                         adhoc = "XY_X", scalar = "XY_XorY")
```

* No data will be excluded.

## Participants

* how are we choosing the sample size? We went for a 20 per cell number. based on?

## Materials and Design

```{r stimuli, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=1.5, set.cap.width=T, num.cols.cap=1, fig.width=1.5,fig.cap = "Cards used in the connective guessing game."}
cards_img <- jpeg::readJPEG("experiments/figs/cards.jpg")
grid::grid.raster(cards_img)
```

* Manipulations: type of card and the type of guess.

The study included six cards with cartoon images of a cat, a dog, and an elephant (Figure \@ref(fig:stimuli)). The study was designed based on the type of cards participants saw and the type of guesses they heard. There were two types of cards: cards with only one animal on them and the ones with two animals. There were three types of guesses: simple (e.g. *There is a cat*), conjunctive (e.g. *There is a cat and a dog*), and disjunctive (e.g. *There is a cat or a dog*). In each guess, the animal labels used in the guess and the animal images on the card may have no overlap (e.g. Image: dog, Guess: *There is a cat or an elephant*), a partial overlap (e.g. Image: Cat, Guess: *There is a cat or an elephant*), or a total overlap (e.g. Image: cat and elephant, Guess: *There is a cat or an elephant*). Crossing the number of animals on the card, the type of guess, and the overlap between the guess and the card results in 12 different possible trial types. We chose 8 trial types (Figure \@ref(fig:trials)), balancing the number of one-animal vs. two-animal cards, simple vs. connective guesses, and expected true vs. false trials. 

* The study used five different types of measurements. 1. two-options (true vs. false) 2. two-options (wrong vs. right) 3. three-options (wrong, neither, right) 4. four-options (wrong, kinda wrong, kinda right, right) 5. five-options (wrong, kinda wrong, neither, kinda right).

```{r trials, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=4, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.width=1.5, fig.cap = "Trial types represented by example cards and guesses."}
trials_img <- jpeg::readJPEG("experiments/figs/trialTypes.jpg")
grid::grid.raster(trials_img)
```

## Procedure

You can access and view the online study using [the following link]().

## Pre-registered Analysis

<!-- changes in the experiment
 - record the response type as response_type
 - record trial type, separate from the card itself and what was said: trial_type, card_type, guess_type, card, guess

-->

<!--We used `r cite_r("r-references.bib")` for all our analyses.-->

* Define implicature rate:

This study set out to test two hypotheses. First, that the proportion of pragmatic vs. literal responses in a truth values judgement task changes based on the number of response options available to the participants. We test this hypothesis formally using a binomial mixed effects model with the fixed effect of response type and the random intercept for participants as well as random intercept and slope for 

Second, ...



# Results

```{r}
results<-
  data %>%
  group_by(response_type, trial_type, response) %>%
  summarize(counts = n()) %>%
  group_by(trial_type, response_type) %>%
  mutate(total = sum(counts), proportion = counts/total)
```

```{r}
ggplot(results, aes(x=response,y=proportion)) +
  geom_bar(stat="identity") +
  facet_grid(response_type~trial_type, drop = TRUE) +
  theme_few()
```

# Analysis

```{r}

implicature_analysis <- glmer(implicature ~ response_type * trial_type + (1+response_type|trial_type) + (1|participant), family="binomial", data=pragmatic_trials)

implicature_analysis <- brm(implicature ~ response_type * trial_type + (1+response_type|trial_type) + (1|participant), family="binomial", data=pragmatic_trials)

```



# Modeling

# Discussion


\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
