---
title             : "The effect of linking assumptions and number of response options on inferred scalar implicature rate"
shorttitle        : "Linking assumptions and implicature rate"
author: 
    
  - name          : "Masoud Jasbi"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "my@email.com"
  - name          : "Brandon Waldon"
    affiliation   : "1"
  - name          : "Judith Degen"
    affiliation   : "1"
affiliation:
  - id            : "1"
    institution   : "Stanford University"
author_note: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.
  Enter author note here.
abstract: |
  Enter abstract here. Each new line herein must be indented, like this line.
  
keywords          : "scalar implicature; methodology; linking assumption; experimental pragmatics; truth-value judgment task"
wordcount         : "X"
bibliography      : ["r-references.bib"]
figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes
mask              : no
class             : "man"
output            : papaja::apa6_pdf
editor_options: 
  chunk_output_type: console
---

```{r load_packages, include = FALSE}
library(grid)
library(gtable)
library(papaja)
library(tidyverse)
library(magrittr)
library(readr)
library(png)
library(jpeg)
library(lme4)
library(ggthemes)
library(forcats)
# library(brms)
theme_set(theme_bw(18))
```

```{r analysis_preferences}
# Seed for random number generation
set.seed(42)
```

# Introduction

The past 15 years have seen the rise and development of a bustling and exciting new field at the intersection of linguistics, psychology, and philosophy: *experimental pragmatics* [@Bott2004;@Breheny2006;@DegenTanenhaus2015;@Grodner2010;@huang2009;@Geurts2009;@noveck2008] **XXX ADD MORE**. Experimental pragmatics is devoted to experimentally testing theories of how language is used in context. How do listeners draw inferences about the -- often underspecified -- linguistic signal they receive from speakers? How do speakers choose between the many utterance alternatives they have at their disposal?

The most prominently studied phenomenon in experimental pragmatics is undoubtedly *scalar implicature*. Scalar implicatures arise in virtue of a speaker producing the weaker of two ordered scalemates [@grice1975; hornXXX; @hirschberg1985; @Geurts2010]. Examples are provided in (1) and (2). 

1. 
  + *Utterance:* Some of her pets are cats.
  + *Implicature:* Some, but not all, of her pets are cats.
  + *Scale:* <all, some>

2. 
  + *Utterance:* She owns a cat or a dog.
  + *Implicature:* She owns a cat or a dog, but not both.
  + *Scale:* <and, or>  
  
A listener, upon observing the utterances in (1a) and (2a), typically infers that the speaker intended to convey the meanings in (1b) and (2b), respectively. Since @grice1975, the agreed-upon abstract rationalization the listener could give for their inference goes something like this: the speaker could have made a more informative statement by producing the stronger alternative (e.g., *All of her pets are cats.*). If the stronger alternative is true, they should have produced it to comply with the Cooperative Principle. They chose not to. I believe the speaker knows whether the stronger alternative is true. Hence, it must not be true.

Because the basic reconstruction of the inference is much more easily characterized for  scalar implicatures than for other implicatures, scalar implicatures have served as a test bed for many questions in experimental pragmatics, including, but not limited to:

1. Are scalar inferences default inferences, in the sense that they arise unless blocked by (marked) contexts [@horn1984; @levinson2000; @Degen2015]?

2. Are scalar inferences default inferences, in the sense that they are computed automatically in online processing and only cancelled by context in a second effortful step if required by context) [@Bott2004;@Breheny2006;@DegenTanenhaus2016;@Grodner2010;@huang2009;@Politzer-Ahles2013;Tomlinson2013]?

3. What are the (linguistic and extra-linguistic) factors that affect whether a scalar implicature is derived [@Zondervan2010;@DegenTanenhaus2015; @DegenTanenhaus2016; @Degen2015; @Degen2014; @Bergen2012; @Breheny2006; @Breheny2013;@DeMarneffe2017;@DeNeys2007;@Bonnefon2009;Chemla2011;Potts2015]?

4. How much diversity is there across implicature types, and within scalar implicatures across scale types, in whether or not an implicature is computed [@Doran2012;@VanTiel2014]?

5. At what age do children acquire the ability to compute implicatures [@Noveck2001; @Papafragou2004; @Barner2011; Frank; @Musolino2004;@Katsos2011]? 

In addressing all of these questions, it has been crucial to obtain estimates of **implicature rates**. For 1., implicature rates from experimental tasks can be taken to inform whether scalar implicatures should be considered default inferences. For 2., processing measures on responses that indicate implicatures can be compared to processing measures on responses that indicate literal interpretations. For 3., contextual effects can be examined by comparing implicature rates across contexts. For 4., implicature rates can be compared across scales (or across implicature types). For 5., implicature rates can be compared across age groups.

A standard measure that has stood proxy for implicature rate across many studies is the proportion of 'pragmatic' judgments in truth-value judgment paradigms [@Bott2004;@Noveck2001;@Noveck2003;@Chemla2011;@Geurts2009;@DegenTanenhaus2015;@DeNeys2007;Degen2014]. In these kinds of tasks, participants are provided a set of facts, either presented visually or via their own knowledge of the world. They are then asked to judge whether a sentence intended to describe those facts is true or false (or alternatively, whether it is right or wrong, or they are asked whether they agree or disagree with the sentence). The crucial condition for assessing implicature rates in these kinds of studies typically consists of a case where the facts are such that the stronger alternative is true and the target utterance is thus also true but underinformative. For instance, @Bott2004 asked participants to judge sentences like 'Some elephants are mammals', when world knowledge dictates that all elephants are mammals. Similarly, @DegenTanenhaus2015 asked participants to judge sentences like 'You got some of the gumballs' in situations where the visual evidence indicated that the participant received all the gumballs from a gumball machine. In these kinds of scenarios, the story goes, if a participant responds 'FALSE', that indicates that they computed a scalar implicature, eg to the effect of 'Not all elephants are mammals' or 'You didn't get all of the gumballs', which is (globally or contextually) false. If instead a participant responds 'TRUE', that is taken to indicate that they interpreted the utterance literally as `Some, and possibly all, elephants are mammals' or 'You got some, and possibly all, of the gumballs'.

Given the centrality of the theoretical notion of 'implicature rate' to much of experimental pragmatics, there is to date a surprising lack of discussion of the basic assumption that it is adequately captured by the  proportion of FALSE responses in truth-value judgment tasks (but see @BenzGotzner; @Geurts2009; @Degen2014; @Katsos2011). Indeed, the scalar implicature acquisition literature was shaken up when @Katsos2011 showed that simply by introducing an additional response option, children started looking much more pragmatic than had been previously observed in a binary judgment paradigm. @Katsos allowed children to distribute 1, 2, or 3 strawberries to a puppet depending on 'how good the puppet said it'. The result was that children gave on average fewer strawberries to the puppet when he produced underinformative utterances compared to when he produced literally true and pragmatically felicitous utterances, suggesting that children do, in fact, display pragmatic ability even at ages when they had previously appeared not to.

But this raises an important question: in truth-value judgment task, how do we know whether an interpretation is literal or the result of an implicature computation? The binary choice task typically used is appealing in part because it allows for a direct mapping from response options -- TRUE and FALSE -- to interpretations -- literal and pragmatic. That the seeming simplicity of this mapping is illusory becomes apparent once a third response option is introduced, as in the @Katsos2011 case. How is the researcher to interpret the intermediate option? @Katsos2011 grouped the intermediate option with the negative endpoint of the scale for the purpose of categorizing judgments as literal vs. pragmatic. But it seems just as plausible that they could have grouped it with the positive endpoint of the scale and taken the hard line that only truly FALSE responses constitute a full-fledged implicature. The point here is that there has been remarkably little consideration of **linking functions** between behavioral measures and theoretical constructs in experimental pragmatics, a problem in many subfields of psycholinguistics @TanenhausXXX. We argue that it is time to engage more seriously with these issues. 

We begin by reporting an experiment that addresses the following question: do the number of response options provided in a truth-value judgment task and the way that responses are grouped into pragmatic ('SI') and literal ('no SI') change inferences about scalar implicature rates? Note that this way of asking the question presupposes two things: first, that whatever participants are doing in a truth-value judgment task, the behavioral measure can be interpreted as providing a measure of **interpretation**. And second, that listeners either do or do not compute an implicature on any given occasion. In the Discussion we will discuss both of these issues. First, following @Degen2014, we will offer some remarks on why truth-value judgment tasks are better thought of as measuring participants' estimates of speakers' **production** probabilities. This will suggest a completely different class of linking functions. And second, we discuss an alternative conception of scalar implicature as a probabilistic phenomeonen, a view that has recently rose to prominence in the subfield of probabilistic pragmatics. This alternative conception of scalar implicature, we argue, affords developing and testing quantitative linking functions in a rigorous and motivated way.

Consider a setup in which a listener is presented a card with a depiction of either one or two animals (see the figure below for an example). As in a standard truth-value judgment task, the listener then observes an underinformative utterance about this card (e.g., 'There is a cat or a dog on the card') and is asked to provide a judgment on a scale from 2 to 5 response options, with endpoints 'wrong' and 'right'. In the binary case, this reproduces the standard truth-value judgment task. **XXX say briefly sth about wrong/right vs true/false and agree/disagree**. The figure below exemplifies (some of) the researcher's options for grouping responses. Under what we will call the 'Strong link' assumption, only the negative endpoint of the scale is interpreted as evidence for a scalar implicature having been computed. Under the 'Weak link' assumption, in contrast, any response that does not correspond to the positive endpoint of the scale is interpreted as evidence for a scalar implicature having been computed. Intermediate grouping schemes are also possible, but these are the ones we will consider here. Note that for the binary case, the Weak and Strong link return the same categorization scheme, but for any number of response options greater than 2, the Weak and Strong link can in principle lead to differences in inferences about implicature rate. 


```{r linkvisualization, fig.env = "figure", fig.align='center', fig.width=5, set.cap.width=T, num.cols.cap=1,fig.cap = "Strong and weak link from response options to researcher inference about scalar implicature rate, exemplified for the disjunctive utterance when the conjunction is true."}
link_img <- png::readPNG("writeup_files/figures/link-visualization.png")
grid::grid.raster(link_img)
```


Let's examine an example. Assume three response options (wrong, neither, right). Assume further that a third of participants each gave each of the three responses, i.e., the distributions of responses is 1/3, 1/3, and 1/3. Under the Strong link, we infer that this task yielded an implicature rate of 2/3. Under the Weak link, we infer that this task yielded an implicature rate of 1/3. This is quite a drastic difference if we are for instance interested in whether scalar implicatures are inference defaults and we would like to interpret an implicature rate of above an arbitrary threshold (e.g., 50%) as evidence for such a claim. Under the Strong link, we would conclude that scalar implicatures are not defaults. Under the Weak link, we would conclude that they are. 


In the experiment reported in the following section, we presented participants with exactly this setup. Different groups of participants were presented with different numbers of response options. We categorized their responses according to the Weak and the Strong link and tested whether number of response options and categorization scheme leads to different conclusions about implicature rates.


# Experiment

## Methods

```{r importData, warning=FALSE, echo=FALSE}
data <- read_csv("experiments/main/3_processed_data/data_main.csv")

data$response_type <- recode(data$response_type, quatenary = "quaternary", tertiary = "ternary")

# define an implicature column for the ad-hoc and implicature trials
pragmatic_trials <- 
  data %>%
  filter(trial_type == "XY_XorY" | trial_type=="XY_X")

# adding a column that defines pragmatic vs. literal

# high definition only considers the highest point on the scale as "literal"
pragmatic_trials$high<-1
pragmatic_trials[pragmatic_trials$response=="Right",]$high <-0

# low definition only considers the lowers point on the scale as implicature 
pragmatic_trials$low<-0
pragmatic_trials[pragmatic_trials$response=="Wrong",]$low <-1

implicature_rate <- 
  pragmatic_trials %>% gather("definition","implicature", high:low)

# changing trial type names to exhaustive vs. scalar
implicature_rate$trial_type <-fct_recode(pragmatic_trials$trial_type, 
                                         exhaustive = "XY_X", scalar = "XY_XorY")
```

###  Participants

```{r participants}
participants_info <-
  data %>%
  group_by(response_type) %>%
  summarise(count = n()/24)
```

200 participants were recruited using Amazon Mechanical Turk (binary=`r participants_info$count[1]`, ternary`r participants_info$count[4]`, quaternary=`r participants_info$count[2]`, quinary=`r participants_info$count[3]`). No participant was excluded from the final analysis.

### Procedure

The study was administered online through Amazon Mechanical Turk. Participants were introduced to a set of cards with pictures of one or two animals (Figure \@ref(fig:stimuli)). They were told that a blindfolded fictional character called Bob is going to guess what animals are on the card. On each trial, participants saw a card as well as a sentence representing Bob's guess. For example, they saw a card with a cat on it and read the sentence "There is a cat on the card." The study ended after 24 trials. At the end participants optionally provided demographic information. You can access and view [the online study here](**XXX masoud insert link**).

### Design and Materials

```{r stimuli, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=1.5, set.cap.width=T, num.cols.cap=1, fig.width=1.5,fig.cap = "Cards used in the connective guessing game."}
cards_img <- jpeg::readJPEG("experiments/figs/cards.jpg")
grid::grid.raster(cards_img)
```

The design had two main manipulaitons: the type of card and the type of guess. There were two types of cards. Cards with only one animal on them and cards with two animals. Animals were chosen from the following set: cat, dog, and elephant  There were three types of guesses: simple (e.g. *There is a cat*), conjunctive (e.g. *There is a cat and a dog*), and disjunctive (e.g. *There is a cat or a dog*).

In each trial, the animal labels used in the guess and the animal images on the card may have no overlap (e.g. Image: dog, Guess: *There is a cat or an elephant*), a partial overlap (e.g. Image: Cat, Guess: *There is a cat or an elephant*), or a total overlap (e.g. Image: cat and elephant, Guess: *There is a cat or an elephant*). Crossing the number of animals on the card, the type of guess, and the overlap between the guess and the card results in 12 different possible trial types. We chose 8 trial types (Figure \@ref(fig:trials)), balancing the number of one-animal vs. two-animal cards, simple vs. connective guesses, and expected true vs. false trials. 

The study used five different types of measurements. 1. two-options (true vs. false) 2. two-options (wrong vs. right) 3. three-options (wrong, neither, right) 4. four-options (wrong, kinda wrong, kinda right, right) 5. five-options (wrong, kinda wrong, neither, kinda right, right).

```{r trials, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=4, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.width=1.5, fig.cap = "Trial types represented by example cards and guesses."}
trials_img <- jpeg::readJPEG("experiments/figs/trialTypes.jpg")
grid::grid.raster(trials_img)
```

## Results

<!--We used `r cite_r("r-references.bib")` for all our analyses.-->

We are primarily concerned with the "rate of implicatures" in an experimental study. Two trial types are predicted to include pragmatic implicatures. First, trials where there are two animals on the card but the fictional character guesses using the connective *or*; for example "cat or dog" when the card has both a cat and a dog on it. We call such trials "scalar" trials. Second, trials where there are two animals on the card but the character guesses only one; for example "cat" when the card has a cat and a dog on it. We call such trials "exhaustive". In our assessment of implicature rate we focus on these two types of trials.

We define "implicature rate" in two ways: 

This study set out to test two hypotheses. First, that the proportion of pragmatic vs. literal responses in a truth values judgement task changes based on the number of response options available to the participants. We test this hypothesis formally using a binomial mixed effects model with the fixed effect of response type and the random intercept for participants as well as random intercept and slope for 

A second hypothesis was that the definition of what responses count as participants computing an implicature may affect the estimated rate of implicature in the experimental task. 


```{r binaryPlot, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=8, fig.height=4, set.cap.width=T, num.cols.cap=1, fig.cap = "Adults' two-alternative forced choice judgments in the connective guessing game."}

binary_summary<-
  data %>%
  filter(response_type=="binary") %>%
  group_by(card_type, guess_type, response) %>%
  summarize(count=n()) %>%
  group_by(card_type, guess_type) %>%
  mutate(total = sum(count), proportion=count/total)

binary_summary$response <- factor(binary_summary$response, levels = c("Wrong","Right"))
binary_summary$guess_type <- factor(binary_summary$guess_type, levels = c("Z","X","XandY","XorY"))
binary_summary$guess_type <- recode(binary_summary$guess_type, Z = "elephant", X = "cat", XandY = "cat and dog", XorY ="cat or dog")

binary_plot<-
  binary_summary %>%
  ggplot(aes(x=response, y=proportion, fill=response)) +
  geom_bar(stat = "identity", position="dodge", width = 0.6) +
#  geom_linerange(aes(ymax=cih, ymin=cil), position= position_dodge(width=0.9)) + 
  facet_grid(card_type~guess_type) +
  labs(x="", y="")+
  theme_few() +
  theme(text = element_text(size=15)) +
  scale_fill_manual(values = c("red4", "springgreen3"), guide=FALSE)

cat <- 
  readJPEG("experiments/figs/cat_card.jpg") %>%
  rasterGrob(width=0.9)

cat_dog <- 
  readJPEG("experiments/figs/catdog_card.jpg")%>%
  rasterGrob(width=0.9)

binary_plot_g <- ggplot_gtable(ggplot_build(binary_plot))

strips <- grep("strip-r", binary_plot_g$layout$name)

new_grobs <- list(cat, cat_dog)

binary_plot_g <- with(binary_plot_g$layout[strips,],
          gtable_add_grob(binary_plot_g, new_grobs,
                          t=t, l=l, b=b, r=r, name="cards"))        
binary_plot_g$widths[[11]] <- unit(2.5,"cm")

grid.draw(binary_plot_g)
```

```{r ternaryPlot, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=8, fig.height=4, set.cap.width=T, num.cols.cap=1, fig.cap = "Adults' three-alternative forced choice judgments in the connective guessing game."}

ternary_summary<-
  data %>%
  filter(response_type=="ternary") %>%
  group_by(card_type, guess_type, response) %>%
  summarize(count=n()) %>%
  group_by(card_type, guess_type) %>%
  mutate(total = sum(count), proportion=count/total)

ternary_summary$response <- factor(ternary_summary$response, levels = c("Wrong","Neither", "Right"))
ternary_summary$guess_type <- factor(ternary_summary$guess_type, levels = c("Z","X","XandY","XorY"))
ternary_summary$guess_type <- recode(ternary_summary$guess_type, Z = "elephant", X = "cat", XandY = "cat and dog", XorY ="cat or dog")

ternary_plot<-
  ternary_summary %>%
  ggplot(aes(x=response, y=proportion, fill=response)) +
  geom_bar(stat = "identity", position="dodge", width = 0.6) +
#  geom_linerange(aes(ymax=cih, ymin=cil), position= position_dodge(width=0.9)) + 
  facet_grid(card_type~guess_type) +
  labs(x="", y="")+
  theme_few() +
  theme(text = element_text(size=15)) +
  scale_fill_manual(values = c("red4","grey", "springgreen3"), guide=FALSE)

cat <- 
  readJPEG("experiments/figs/cat_card.jpg") %>%
  rasterGrob(width=0.9)

cat_dog <- 
  readJPEG("experiments/figs/catdog_card.jpg")%>%
  rasterGrob(width=0.9)

ternary_plot_g <- ggplot_gtable(ggplot_build(ternary_plot))

strips <- grep("strip-r", ternary_plot_g$layout$name)

new_grobs <- list(cat, cat_dog)

ternary_plot_g <- with(ternary_plot_g$layout[strips,],
          gtable_add_grob(ternary_plot_g, new_grobs,
                          t=t, l=l, b=b, r=r, name="cards"))        
ternary_plot_g$widths[[11]] <- unit(2.5,"cm")

grid.draw(ternary_plot_g)
```

```{r quaternaryPlot, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=8, fig.height=4, set.cap.width=T, num.cols.cap=1, fig.cap = "Adults' three-alternative forced choice judgments in the connective guessing game."}

quaternary_summary<-
  data %>%
  filter(response_type=="quaternary") %>%
  group_by(card_type, guess_type, response) %>%
  summarize(count=n()) %>%
  group_by(card_type, guess_type) %>%
  mutate(total = sum(count), proportion=count/total)

quaternary_summary$response <- factor(quaternary_summary$response, levels = c("Wrong","Kinda Wrong", "Kinda Right", "Right"))
quaternary_summary$guess_type <- factor(quaternary_summary$guess_type, levels = c("Z","X","XandY","XorY"))
quaternary_summary$guess_type <- recode(quaternary_summary$guess_type, Z = "elephant", X = "cat", XandY = "cat and dog", XorY ="cat or dog")

quaternary_plot<-
  quaternary_summary %>%
  ggplot(aes(x=response, y=proportion, fill=response)) +
  geom_bar(stat = "identity", position="dodge", width = 0.6) +
#  geom_linerange(aes(ymax=cih, ymin=cil), position= position_dodge(width=0.9)) + 
  facet_grid(card_type~guess_type) +
  labs(x="", y="")+
  theme_few() +
  theme(text = element_text(size=17), axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_fill_manual(values = c("red4", "red3", "springgreen2", "springgreen3"), guide=FALSE)

cat <- 
  readJPEG("experiments/figs/cat_card.jpg") %>%
  rasterGrob(width=0.9)

cat_dog <- 
  readJPEG("experiments/figs/catdog_card.jpg")%>%
  rasterGrob(width=0.9)

quaternary_plot_g <- ggplot_gtable(ggplot_build(quaternary_plot))

strips <- grep("strip-r", quaternary_plot_g$layout$name)

new_grobs <- list(cat, cat_dog)

quaternary_plot_g <- with(quaternary_plot_g$layout[strips,],
          gtable_add_grob(quaternary_plot_g, new_grobs,
                          t=t, l=l, b=b, r=r, name="cards"))        
quaternary_plot_g$widths[[11]] <- unit(2.5,"cm")

grid.draw(quaternary_plot_g)
```

```{r quinaryPlot, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=8, fig.height=4, set.cap.width=T, num.cols.cap=1, fig.cap = "Adults' three-alternative forced choice judgments in the connective guessing game."}

quinary_summary<-
  data %>%
  filter(response_type=="quinary") %>%
  group_by(card_type, guess_type, response) %>%
  summarize(count=n()) %>%
  group_by(card_type, guess_type) %>%
  mutate(total = sum(count), proportion=count/total)

quinary_summary$response <- factor(quinary_summary$response, levels = c("Wrong","Kinda Wrong", "Neither", "Kinda Right", "Right"))
quinary_summary$guess_type <- factor(quinary_summary$guess_type, levels = c("Z","X","XandY","XorY"))
quinary_summary$guess_type <- recode(quinary_summary$guess_type, Z = "elephant", X = "cat", XandY = "cat and dog", XorY ="cat or dog")

quinary_plot<-
  quinary_summary %>%
  ggplot(aes(x=response, y=proportion, fill=response)) +
  geom_bar(stat = "identity", position="dodge", width = 0.6) +
#  geom_linerange(aes(ymax=cih, ymin=cil), position= position_dodge(width=0.9)) + 
  facet_grid(card_type~guess_type) +
  labs(x="", y="")+
  theme_few() +
  theme(text = element_text(size=17), axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_fill_manual(values = c("red4", "red3", "grey", "springgreen2", "springgreen3"), guide=FALSE)

cat <- 
  readJPEG("experiments/figs/cat_card.jpg") %>%
  rasterGrob(width=0.9)

cat_dog <- 
  readJPEG("experiments/figs/catdog_card.jpg")%>%
  rasterGrob(width=0.9)

quinary_plot_g <- ggplot_gtable(ggplot_build(quinary_plot))

strips <- grep("strip-r", quinary_plot_g$layout$name)

new_grobs <- list(cat, cat_dog)

quinary_plot_g <- with(quinary_plot_g$layout[strips,],
          gtable_add_grob(quinary_plot_g, new_grobs,
                          t=t, l=l, b=b, r=r, name="cards"))        
quinary_plot_g$widths[[11]] <- unit(2.5,"cm")

grid.draw(quinary_plot_g)
```


```{r}
results<-
  data %>%
  group_by(response_type, trial_type, response) %>%
  summarize(counts = n()) %>%
  group_by(trial_type, response_type) %>%
  mutate(total = sum(counts), proportion = counts/total)
```

```{r}
ggplot(results, aes(x=response,y=proportion)) +
  geom_bar(stat="identity") +
  facet_grid(response_type~trial_type, drop = TRUE) +
  theme_few()
```

* make sure to break down based on whether participants had logical training or not.


### Analysis

```{r implicatureResultss}
implicature_results<-
  implicature_rate %>%
  group_by(response_type, trial_type, implicature, definition) %>%
  summarize(counts = n()) %>%
  group_by(response_type, trial_type, definition) %>%
  mutate(total = sum(counts), proportion = counts/total)
```

```{r}
ggplot(implicature_results, aes(x=as.factor(implicature),y=proportion)) +
  geom_bar(stat="identity", position = "dodge") +
  facet_grid(response_type~definition + trial_type, drop = TRUE) +
  theme_few() + 
  labs(x="Implicature")
```


```{r}
implicature_rate$card <- implicature_rate$card %>% tolower()

implicature_analysis_a <- glmer(implicature ~ definition * response_type + trial_type + (1+response_type|card) + (1|participant), family="binomial", data=implicature_rate)

summary(implicature_analysis_a)

# implicature_analysis_b <- brm(implicature ~ definition * response_type + trial_type + (1+response_type|trial_type) + (1|participant), family="binomial", data=implicature_rate)
```



# Discussion


Alternative Linking Hypothesis:
* RSA: Response behavior across conditions (utterance-card combinations) and dependent measures can be predicted by a linking hypothesis that assumes that participants are behaving like soft-optimal RSA speakers and provide a particular response (eg TRUE) to an utterance u if the RSA speaker probability of u (given the card) is within a particular probability interval (eg, within the interval [theta, 1]).

* Differences between traditional approaches and RSA: 1. The traditional linking hypotheses are based on a binary implicature/literal theory of pragmatic reasoning but RSA gives a continuous measure of pragmatic reasoning and allows for better predicting response behavior with multiple options.

A simple visual inspection of our results suggests that any variant of the Traditional Linking Hypothesis will yield unsatisfying empirical coverage with regards to participants’ behavior in truth value judgment tasks. Recall that according to this hypothesis, scalar implicature is conceptualized as a binary, categorical affair: an implicature is either ‘calculated,’ or it isn’t. This assumption has implications for how we approach analysis of variation in behavior on a truth value judgment task; for example, why did the majority of respondents in the binary condition of our experiment answer “Right” to an utterance of cat or dog when the card had a cat and a dog on it? 

On the Traditional story, we are forced to say that a) not all participants calculated the implicature; or that b) some participants who calculated the implicature did not choose the anticipated response (i.e. “Wrong”)  due to some other cognitive reflex which ‘overrode’ the implicature; or some mixture of (b) and (c). We might similarly posit that one or both of these factors underlie the variation in the ternary, quatenary, and quinary conditions (e.g. why were participants roughly split between “Right” and “Kind of right” when the utterance was cat or dog and the card had a cat and a dog?). There is an established cottage industry of pragmatics research which approaches the problem of behavioral variation in exactly these ways, exploring the factors which “license” implicature calculation or “suppress” its anticipated behavioral expression (c.f. __), but the best we can hope for on this approach is an analysis which traces the general qualitative patterns in the data (and we should stress that even in this respect, the industry has yet to fully deliver the goods!).

The Alternative Linking Hypothesis contrasts with the Traditional Linking Hypothesis in that the former is rooted in a quantitative formalization of pragmatic competence which provides us a continuous measure of pragmatic reasoning. [recap RSA model here? Maybe show how it gets the right basic qualitative pattern, i.e. $P_L$(a and b| “a or b”) $<$ $P_L$(a | “or”); $P_L$(a and b| “a or b”) $<$ $P_L$(b | “or”) ]

Following Degen & Goodman (2014), we proceed on the assumption that behavior on sentence verification tasks, such as truth value judgment tasks, is best modeled as a function of an individual’s mental representation of a cooperative interlocutor ($S_1$ in the language of RSA). For a given utterance u and an intended communicated meaning w, $S_1$(u | w) outputs a conditional probability of u given w. For example, in the binary condition of our experiment where a participant evaluated cat or dog when there were both animals on the card, the participant has access to the mental representation of $S_1$ and hence to the $S_1$ conditional probability of hearing the utterance cat or dog given a dog and cat card $S_1$(cat or dog | cat and dog). 

One desideratum of a linking hypothesis given this view of pragmatic competence is to transform this probability value into a categorical output (e.g. “Right”/”Wrong” in the case of the binary condition). This can be achieved by positing that participants ‘filter out’ low-probability utterances given an intended communicated meaning. We model a responder, R, who in the binary condition responds “Right” to an utterance u in world w just in case $S_1$(u | w) exceeds some probability threshold $\theta$: 

R(u, w, $\theta$) 
    = “Right” iff $S_1$(u | w) $>$ $\theta$
    = “Wrong” otherwise 

In the experiment conditions where there are more than two choices, we model the range of possible behavioral responses for R with the introduction of intermediate probability thresholds. For example, in the ternary condition, R(u, w, $\theta_1$ , $\theta_2$)  is “Right” iff $S_1$(u | w) > $\theta_1$ and “Neither” iff $\theta_1$ > $S_1$(u | w) > $\theta_2$. To fully generalize the model to our five experimental conditions, we say that R takes as its input an utterance u and a world state w and a number of threshold variables dependent on a variable c, corresponding to the experimental condition in which the participant finds herself (e.g. the range of possible responses available to R). 

Given c = “ternary” 

R(u, w, $\theta_1$ , $\theta_2$)  
    = “Right” iff $S_1$(u | w) $>$ $\theta_1$
    = “Neither” iff $\theta_1$ $>$ $S_1$(u | w) $>$ $\theta_2$  
    = “Wrong” otherwise 

Given c = “quatenary” 

R(u, w, $\theta_1$ , $\theta_2$, $\theta_3$)  
    = “Right” iff $S_1$(u | w) $>$ $\theta_1$
    = “Kinda Right” iff $\theta_1$ $>$ $S_1$(u | w) $>$ $\theta_2$  
    = “Kinda Wrong” iff $\theta_2$ $>$ $S_1$(u | w) $>$ $\theta_3$ 
    = “Wrong” otherwise 

Given c = “quinary” 

R(u, w, $\theta_1$ , $\theta_2$, $\theta_3$. $\theta_4$)  
    = “Right” iff $S_1$(u | w) $>$ $\theta_1$
    =“Kinda Right” iff $\theta_1$ $>$ $S_1$(u | w) $>$ $\theta_2$ 
    = “Neither” iff $\theta_2$ $>$ $S_1$(u | w) $>$ $\theta_3$  
    = “Kinda Wrong” iff $\theta_3$ $>$ $S_1$(u | w) $>$ $\theta_4$ 
    = “Wrong” otherwise 

\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
