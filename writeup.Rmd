---
title             : "The effect of linking assumptions and number of response options on inferred scalar implicature rate"
shorttitle        : "Linking assumptions and implicature rate"
author: 
    
  - name          : "Masoud Jasbi"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "my@email.com"
  - name          : "Brandon Waldon"
    affiliation   : "1"
  - name          : "Judith Degen"
    affiliation   : "1"
affiliation:
  - id            : "1"
    institution   : "Stanford University"
author_note: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.
  Enter author note here.
abstract: |
  Enter abstract here. Each new line herein must be indented, like this line.
  
keywords          : "scalar implicature; methodology; linking assumption; experimental pragmatics; truth-value judgment task"
wordcount         : "X"
bibliography      : ["r-references.bib"]
figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes
mask              : no
class             : "man"
output            : papaja::apa6_pdf
editor_options: 
  chunk_output_type: inline
---

```{r load_packages, include = FALSE}
library(grid)
library(gtable)
library(papaja)
library(tidyverse)
library(magrittr)
library(readr)
library(png)
library(jpeg)
library(lme4)
library(ggthemes)
library(forcats)
library(brms)
library(DescTools)
library(binom)
library(knitr)
theme_set(theme_bw(18))
```

```{r analysis_preferences}
# Seed for random number generation
set.seed(42)
```

# Introduction

The past 15 years have seen the rise and development of a bustling and exciting new field at the intersection of linguistics, psychology, and philosophy: *experimental pragmatics* [@Bott2004;@Breheny2006;@DegenTanenhaus2015;@Grodner2010;@huang2009;@Geurts2009;@noveck2008] **XXX ADD MORE**. Experimental pragmatics is devoted to experimentally testing theories of how language is used in context. How do listeners draw inferences about the -- often underspecified -- linguistic signal they receive from speakers? How do speakers choose between the many utterance alternatives they have at their disposal?

The most prominently studied phenomenon in experimental pragmatics is undoubtedly *scalar implicature*. Scalar implicatures arise in virtue of a speaker producing the weaker of two ordered scalemates [@grice1975; hornXXX; @hirschberg1985; @Geurts2010]. Examples are provided in (1) and (2). 

1. 
  + *Utterance:* Some of her pets are cats.
  + *Implicature:* Some, but not all, of her pets are cats.
  + *Scale:* <all, some>

2. 
  + *Utterance:* She owns a cat or a dog.
  + *Implicature:* She owns a cat or a dog, but not both.
  + *Scale:* <and, or>  
  
A listener, upon observing the utterances in (1a) and (2a), typically infers that the speaker intended to convey the meanings in (1b) and (2b), respectively. Since @grice1975, the agreed-upon abstract rationalization the listener could give for their inference goes something like this: the speaker could have made a more informative statement by producing the stronger alternative (e.g., *All of her pets are cats.*). If the stronger alternative is true, they should have produced it to comply with the Cooperative Principle. They chose not to. I believe the speaker knows whether the stronger alternative is true. Hence, it must not be true.

Because the basic reconstruction of the inference is much more easily characterized for  scalar implicatures than for other implicatures, scalar implicatures have served as a test bed for many questions in experimental pragmatics, including, but not limited to:

1. Are scalar inferences default inferences, in the sense that they arise unless blocked by (marked) contexts [@horn1984; @levinson2000; @Degen2015]?

2. Are scalar inferences default inferences, in the sense that they are computed automatically in online processing and only cancelled by context in a second effortful step if required by context) [@Bott2004;@Breheny2006;@DegenTanenhaus2016;@Grodner2010;@huang2009;@Politzer-Ahles2013;Tomlinson2013]?

3. What are the (linguistic and extra-linguistic) factors that affect whether a scalar implicature is derived [@Zondervan2010;@DegenTanenhaus2015; @DegenTanenhaus2016; @Degen2015; @Degen2014; @Bergen2012; @Breheny2006; @Breheny2013;@DeMarneffe2017;@DeNeys2007;@Bonnefon2009;Chemla2011;Potts2015]?

4. How much diversity is there across implicature types, and within scalar implicatures across scale types, in whether or not an implicature is computed [@Doran2012;@VanTiel2014]?

5. At what age do children acquire the ability to compute implicatures [@Noveck2001; @Papafragou2004; @Barner2011; Frank; @Musolino2004;@Katsos2011]? 

In addressing all of these questions, it has been crucial to obtain estimates of **implicature rates**. For 1., implicature rates from experimental tasks can be taken to inform whether scalar implicatures should be considered default inferences. For 2., processing measures on responses that indicate implicatures can be compared to processing measures on responses that indicate literal interpretations. For 3., contextual effects can be examined by comparing implicature rates across contexts. For 4., implicature rates can be compared across scales (or across implicature types). For 5., implicature rates can be compared across age groups.

A standard measure that has stood proxy for implicature rate across many studies is the proportion of 'pragmatic' judgments in truth-value judgment paradigms [@Bott2004;@Noveck2001;@Noveck2003;@Chemla2011;@Geurts2009;@DegenTanenhaus2015;@DeNeys2007;Degen2014]. In these kinds of tasks, participants are provided a set of facts, either presented visually or via their own knowledge of the world. They are then asked to judge whether a sentence intended to describe those facts is true or false (or alternatively, whether it is right or wrong, or they are asked whether they agree or disagree with the sentence). The crucial condition for assessing implicature rates in these kinds of studies typically consists of a case where the facts are such that the stronger alternative is true and the target utterance is thus also true but underinformative. For instance, @Bott2004 asked participants to judge sentences like 'Some elephants are mammals', when world knowledge dictates that all elephants are mammals. Similarly, @DegenTanenhaus2015 asked participants to judge sentences like 'You got some of the gumballs' in situations where the visual evidence indicated that the participant received all the gumballs from a gumball machine. In these kinds of scenarios, the story goes, if a participant responds 'FALSE', that indicates that they computed a scalar implicature, eg to the effect of 'Not all elephants are mammals' or 'You didn't get all of the gumballs', which is (globally or contextually) false. If instead a participant responds 'TRUE', that is taken to indicate that they interpreted the utterance literally as `Some, and possibly all, elephants are mammals' or 'You got some, and possibly all, of the gumballs'.

Given the centrality of the theoretical notion of 'implicature rate' to much of experimental pragmatics, there is to date a surprising lack of discussion of the basic assumption that it is adequately captured by the  proportion of FALSE responses in truth-value judgment tasks (but see @BenzGotzner; @Geurts2009; @Degen2014; @Katsos2011). Indeed, the scalar implicature acquisition literature was shaken up when @Katsos2011 showed that simply by introducing an additional response option, children started looking much more pragmatic than had been previously observed in a binary judgment paradigm. @Katsos allowed children to distribute 1, 2, or 3 strawberries to a puppet depending on 'how good the puppet said it'. The result was that children gave on average fewer strawberries to the puppet when he produced underinformative utterances compared to when he produced literally true and pragmatically felicitous utterances, suggesting that children do, in fact, display pragmatic ability even at ages when they had previously appeared not to.

But this raises an important question: in truth-value judgment task, how do we know whether an interpretation is literal or the result of an implicature computation? The binary choice task typically used is appealing in part because it allows for a direct mapping from response options -- TRUE and FALSE -- to interpretations -- literal and pragmatic. That the seeming simplicity of this mapping is illusory becomes apparent once a third response option is introduced, as in the @Katsos2011 case. How is the researcher to interpret the intermediate option? @Katsos2011 grouped the intermediate option with the negative endpoint of the scale for the purpose of categorizing judgments as literal vs. pragmatic. But it seems just as plausible that they could have grouped it with the positive endpoint of the scale and taken the hard line that only truly FALSE responses constitute a full-fledged implicature. The point here is that there has been remarkably little consideration of **linking functions** between behavioral measures and theoretical constructs in experimental pragmatics, a problem in many subfields of psycholinguistics @TanenhausXXX. We argue that it is time to engage more seriously with these issues. 

We begin by reporting an experiment that addresses the following question: do the number of response options provided in a truth-value judgment task and the way that responses are grouped into pragmatic ('SI') and literal ('no SI') change inferences about scalar implicature rates? Note that this way of asking the question presupposes two things: first, that whatever participants are doing in a truth-value judgment task, the behavioral measure can be interpreted as providing a measure of **interpretation**. And second, that listeners either do or do not compute an implicature on any given occasion. In the Discussion we will discuss both of these issues. First, following @Degen2014, we will offer some remarks on why truth-value judgment tasks are better thought of as measuring participants' estimates of speakers' **production** probabilities. This will suggest a completely different class of linking functions. And second, we discuss an alternative conception of scalar implicature as a probabilistic phenomeonen, a view that has recently rose to prominence in the subfield of probabilistic pragmatics. This alternative conception of scalar implicature, we argue, affords developing and testing quantitative linking functions in a rigorous and motivated way.

Consider a setup in which a listener is presented a card with a depiction of either one or two animals (see the figure below for an example). As in a standard truth-value judgment task, the listener then observes an underinformative utterance about this card (e.g., 'There is a cat or a dog on the card') and is asked to provide a judgment on a scale from 2 to 5 response options, with endpoints 'wrong' and 'right'. In the binary case, this reproduces the standard truth-value judgment task. **XXX say briefly sth about wrong/right vs true/false and agree/disagree**. The figure below exemplifies (some of) the researcher's options for grouping responses. Under what we will call the 'Strong link' assumption, only the negative endpoint of the scale is interpreted as evidence for a scalar implicature having been computed. Under the 'Weak link' assumption, in contrast, any response that does not correspond to the positive endpoint of the scale is interpreted as evidence for a scalar implicature having been computed. Intermediate grouping schemes are also possible, but these are the ones we will consider here. Note that for the binary case, the Weak and Strong link return the same categorization scheme, but for any number of response options greater than 2, the Weak and Strong link can in principle lead to differences in inferences about implicature rate. 

```{r linkvisualization, fig.asp=0.2,fig.cap = "Strong and weak link from response options to researcher inference about scalar implicature rate, exemplified for the disjunctive utterance when the conjunction is true."}
link_img <- png::readPNG("writeup_files/figures/link-visualization.png")
grid::grid.raster(link_img)
```

Let's examine an example. Assume three response options (wrong, neither, right). Assume further that a third of participants each gave each of the three responses, i.e., the distributions of responses is 1/3, 1/3, and 1/3. Under the Strong link, we infer that this task yielded an implicature rate of 2/3. Under the Weak link, we infer that this task yielded an implicature rate of 1/3. This is quite a drastic difference if we are for instance interested in whether scalar implicatures are inference defaults and we would like to interpret an implicature rate of above an arbitrary threshold (e.g., 50%) as evidence for such a claim. Under the Strong link, we would conclude that scalar implicatures are not defaults. Under the Weak link, we would conclude that they are. In the experiment reported in the following section, we presented participants with exactly this setup. 


# Experiment

In this study, we presented participants with an online card game. Different groups of participants were presented with different numbers of response options for the task. In critical trials, participants were presented with descriptions for the cards that typically result in exhaustive or scalar implicatures. We categorized their responses in such trials according to the "weak" and the "strong" link, and tested whether the number of response options and the linking assumptions lead to different conclusions about the rate of computed implicatures in the experimental task.

## Methods

```{r importData, warning=FALSE, echo=FALSE}
data <- read_csv("experiments/main/3_processed_data/data_main.csv")

data$response_type <- recode(data$response_type, quatenary = "quaternary", tertiary = "ternary")

# define an implicature column for the ad-hoc and implicature trials
pragmatic_trials <- 
  data %>%
  filter(trial_type == "XY_XorY" | trial_type=="XY_X")

# adding a column that defines pragmatic vs. literal

# weak definition only considers the highest point on the scale as "literal" (Weak link in paper)
pragmatic_trials$weak<-1
pragmatic_trials[pragmatic_trials$response=="Right",]$weak <-0

# strong definition only considers the lowest point on the scale as implicature (Strong link in paper)
pragmatic_trials$strong<-0
pragmatic_trials[pragmatic_trials$response=="Wrong",]$strong <-1

implicature_rate <- 
  pragmatic_trials %>% gather("definition","implicature", weak:strong)

# changing trial type names to exhaustive vs. scalar
implicature_rate$trial_type <-fct_recode(pragmatic_trials$trial_type, 
                                         exhaustive = "XY_X", scalar = "XY_XorY")
```

###  Participants

```{r participants}
participants_info <-
  data %>%
  group_by(response_type) %>%
  summarise(count = n()/24)

mean_age <- mean(data$age)

logic <- data %>% select(participant, logical_training) %>% unique()

logicTraining <- table(logic$logical_training)
```

200 participants were recruited using Amazon Mechanical Turk. They optionally provided demographic information at the end of the study. Mean age of the participnats was `r round(mean_age)`. We also asked participants if they had any prior training in logic. `r logicTraining[[2]]` participants reported that they had while `r logicTraining[[1]]` had no prior training in logic. No participant was excluded from the final analysis.

### Materials and Procedure

The study was administered online through Amazon Mechanical Turk. Participants were first introduced to the set of cards we used in the study (Figure \@ref(fig:stimuli)). Each card had pictures of one or two animals. Animals were chosen from the following set: cat, dog, and elephant. Then participants were introduced to a blindfolded fictional character called Bob. Bob was blindfolded to control for violations of ignorance expectations with disjunction. Participants were told that Bob is going to guess what animals are on the card. We asked participants to let us know whether Bob's guess is wrong or right. In each trial, participants saw a card as well as a sentence representing Bob's guess. For example, they saw a card with a cat and read the sentence "There is a cat on the card." Depending on the task participants were assigned to, they had to choose between two (binary task), three (ternary task), four (quaternary task), or five (quinary task) response options. The study ended after 24 trials. You can access and view the study using the paper's [online repository](https://github.com/thegricean/si-paradigms/).

### Design

```{r stimuli, fig.asp=0.35, fig.cap = "Cards used in the connective guessing game."}
cards_img <- jpeg::readJPEG("experiments/figs/cards.jpg")
grid::grid.raster(cards_img)
```

The study had two main manipulaitons within participants: the type of card and the type of guess. There were two types of cards. Cards with only one animal on them and cards with two animals. There were three types of guesses: simple (e.g. *There is a cat*), conjunctive (e.g. *There is a cat and a dog*), and disjunctive (e.g. *There is a cat or a dog*). In each trial, the animal labels used in the guess and the animal images on the card may have no overlap (e.g. Image: cat, Guess: *There is an elephant*), a partial overlap (e.g. Image: cat, Guess: *There is a cat or a dog*), or a total overlap (e.g. Image: cat and dog, Guess: *There is a cat or a dog*). Crossing the number of animals on the card, the type of guess, and the overlap between the guess and the card results in 12 different possible trial types. We chose 8 trial types, balancing the number of one-animal vs. two-animal cards, simple vs. connective guesses, and expected true vs. false trials. Three trials were randomly selected from each of the 8 trial-types, for a total of 24 trials. The order of these 24 trials was randomized as well. 

Participants could derive implicatures in two trial types. First, the trial type in which two animals were present on the card (e.g. cat and dog) but Bob guessed only one of them (e.g. "there is a cat"). Such trials can have a literal interpretation (cat and possibly more) or an exhaustive interpretation (only cat). We refer to them as "exhaustive". The second trial type with implicatures was the one in which two animals were on the card (e.g. cat and dog) and Bob used a disjunciton (e.g. cat or dog). These trials can have a literal (inclusive) interpretation (e.g. cat or dog or both), or an exclusive interpretation (e.g. cat or dog, not both). We refer to these trials as "scalar". The following four trial types were used as experimental control: two trial types where there was no overlap between the guess (e.g. elephant) and the animal(s) on the card (e.g. cat, cat and dog); and two trial types where the animal(s) on the card were correctly guessed. For example, if there was only a cat on the card, Bob said "there is a cat" and if there was a cat and a dog, Bob said "there is a cat and a dog". Since the fictional character was blindfolded and did not see the outcome of the game, the ignorance inference commonly associated with disjunction was already common ground in the experimental setting. If the character was seeing the cards or knew what was on them, a disjunction would have violated the expectation that the speaker does not know which alternative actually holds. Our study controls for the possibile effect of ignorance violations on exclusivity and exhaustive inferences. 

The study also had a between participant manipulation of the number of response options in the forced choice task. Participants were randomly assigned to one of four different conditions or tasks. The tasks differed with respect to the number of response options: binary (wrong vs. right), ternary (wrong, neither, right), quaternary (wrong, kinda wrong, kinda right, right), and quinary (wrong, kinda wrong, neither, kinda right, right). We wanted to see if the number of response options in the forced choice task would affect our estimate of the task's "implicature rate".

## Results

```{r binaryData}
binary_summary<-
  data %>%
  filter(response_type=="binary") %>%
  group_by(card_type, guess_type, response) %>%
  summarize(count=n()) %>%
  group_by(card_type, guess_type) %>%
  mutate(total = sum(count), est=count/total)

binary_summary_X_cat <-
  binary_summary %>%
  filter(card_type=="X", guess_type=="X")
binary_summary_X_cat_confint <-
  binary_summary_X_cat$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
binary_summary_X_cat <- binary_summary_X_cat %>% full_join(binary_summary_X_cat_confint, by="est")

binary_summary_XY_cat <-
  binary_summary %>%
  filter(card_type=="XY", guess_type=="X")
binary_summary_XY_cat_confint <-
  binary_summary_XY_cat$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
binary_summary_XY_cat <- binary_summary_XY_cat %>% full_join(binary_summary_XY_cat_confint, by="est")

binary_summary_XY_ele <-
  binary_summary %>%
  filter(card_type=="XY", guess_type=="Z")
binary_summary_XY_ele_confint <-
  binary_summary_XY_ele$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
binary_summary_XY_ele <- binary_summary_XY_ele %>% full_join(binary_summary_XY_ele_confint, by="est") %>% unique()

binary_summary_X_ele <-
  binary_summary %>%
  filter(card_type=="X", guess_type=="Z")
binary_summary_X_ele_confint <-
  binary_summary_X_ele$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
binary_summary_X_ele <- binary_summary_X_ele %>% full_join(binary_summary_X_ele_confint, by="est") %>% unique()

binary_summary_XY_and <-
  binary_summary %>%
  filter(card_type=="XY", guess_type=="XandY")
binary_summary_XY_and_confint <-
  binary_summary_XY_and$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
binary_summary_XY_and <- binary_summary_XY_and %>% full_join(binary_summary_XY_and_confint, by="est") %>% unique()

binary_summary_X_and <-
  binary_summary %>%
  filter(card_type=="X", guess_type=="XandY")
binary_summary_X_and_confint <-
  binary_summary_X_and$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
binary_summary_X_and <- binary_summary_X_and %>% full_join(binary_summary_X_and_confint, by="est") %>% unique()

binary_summary_XY_or <-
  binary_summary %>%
  filter(card_type=="XY", guess_type=="XorY")
binary_summary_XY_or_confint <-
  binary_summary_XY_or$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
binary_summary_XY_or <- binary_summary_XY_or %>% full_join(binary_summary_XY_or_confint, by="est") %>% unique()

binary_summary_X_or <-
  binary_summary %>%
  filter(card_type=="X", guess_type=="XorY")
binary_summary_X_or_confint <-
  binary_summary_X_or$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
binary_summary_X_or <- binary_summary_X_or %>% full_join(binary_summary_X_or_confint, by="est") %>% unique()

binary_summary <- bind_rows(binary_summary_X_cat, binary_summary_XY_cat, binary_summary_XY_ele, binary_summary_X_ele, binary_summary_XY_and, binary_summary_X_and, binary_summary_XY_or, binary_summary_X_or)

binary_summary$response <- factor(binary_summary$response, levels = c("Wrong","Right"))
binary_summary$guess_type <- factor(binary_summary$guess_type, levels = c("Z","X","XandY","XorY"))
binary_summary$guess_type <- recode(binary_summary$guess_type, Z = "elephant", X = "cat", XandY = "cat and dog", XorY ="cat or dog")
binary_summary <- binary_summary %>% rename(proportion = "est")
```

```{r binaryPlot, fig.height=3.5, fig.cap = "Proportion responses for the two-alternative (binary) forced choice judgments in the guessing game."}
binary_plot<-
  binary_summary %>%
  ggplot(aes(x=response, y=proportion, fill=response)) +
  geom_bar(stat = "identity", position="dodge", width = 0.6) +
  facet_grid(card_type~guess_type) +
  labs(x="Response Options in the Binary Task", y="Proportions")+
  theme_few() +
  theme(text = element_text(size=12)) +
  scale_fill_manual(values = c("red4", "springgreen3"), guide=FALSE) +
  geom_linerange(aes(ymin=lwr.ci,ymax=upr.ci))

cat <- 
  readJPEG("experiments/figs/cat_card.jpg") %>%
  rasterGrob(width=0.9)

cat_dog <- 
  readJPEG("experiments/figs/catdog_card.jpg")%>%
  rasterGrob(width=0.9)

binary_plot_g <- ggplot_gtable(ggplot_build(binary_plot))

strips <- grep("strip-r", binary_plot_g$layout$name)

new_grobs <- list(cat, cat_dog)

binary_plot_g <- with(binary_plot_g$layout[strips,],
          gtable_add_grob(binary_plot_g, new_grobs,
                          t=t, l=l, b=b, r=r, name="cards"))        
binary_plot_g$widths[[11]] <- unit(2.5,"cm")

grid.draw(binary_plot_g)

binaryExh_wrong <- binary_summary %>% filter(card_type=="XY", guess_type=="cat", response=="Wrong")
binaryScalar_wrong <- binary_summary %>% filter(card_type=="XY", guess_type=="cat or dog", response=="Wrong")
```

The experiment had `r participants_info$count[1]` participants in the binary task, `r participants_info$count[4]` in the ternary task, `r participants_info$count[2]` in the quaternary task and `r participants_info$count[3]` in the quinary task. In this section, we present the proportion that participants chose different response options in each of the 8 trial types of these four tasks. 

Figure \@ref(fig:binaryPlot) shows the proportion of "right" and "wrong" responses in the binary task. Starting from the leftmost column, participants considered a guess "wrong" if the guessed animal was not on the card. Moving to the second column, participants considered the guess "right" when the only animal on the card was mentioned. However, when only one of the two animals on the card was mentioned (exhaustive trials), `r round(binaryExh_wrong$proportion*100)`% of the times participants considered the guess "wrong". Moving to the third column, if a conjunction of animals was guessed while only one animal was on the card, participants considered the guess to be "wrong". If a conjunction of animals was guessed and both animals were present on the card, all participants considered the guess to be "right" as expected. Finally, if a disjunction of animals was guessed and only one of the animals was on the card, participants considered the guess to be "right" almost all the time. However, if both animals were present (scalar trials), `r round(binaryScalar_wrong$proportion*100)`% of the times participants considered the guess to be "wrong".

```{r ternaryData}
ternary_summary<-
  data %>%
  filter(response_type=="ternary") %>%
  group_by(card_type, guess_type, response) %>%
  summarize(count=n()) %>%
  group_by(card_type, guess_type) %>%
  mutate(total = sum(count), est=count/total)

ternary_summary_X_cat <-
  ternary_summary %>%
  filter(card_type=="X", guess_type=="X")
ternary_summary_X_cat_confint <-
  ternary_summary_X_cat$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
ternary_summary_X_cat <- ternary_summary_X_cat %>% full_join(ternary_summary_X_cat_confint, by="est")

ternary_summary_XY_cat <-
  ternary_summary %>%
  filter(card_type=="XY", guess_type=="X")
ternary_summary_XY_cat_confint <-
  ternary_summary_XY_cat$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
ternary_summary_XY_cat <- ternary_summary_XY_cat %>% full_join(ternary_summary_XY_cat_confint, by="est")

ternary_summary_XY_ele <-
  ternary_summary %>%
  filter(card_type=="XY", guess_type=="Z")
ternary_summary_XY_ele_confint <-
  ternary_summary_XY_ele$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
ternary_summary_XY_ele <- ternary_summary_XY_ele %>% full_join(ternary_summary_XY_ele_confint, by="est") %>% unique()

ternary_summary_X_ele <-
  ternary_summary %>%
  filter(card_type=="X", guess_type=="Z")
ternary_summary_X_ele_confint <-
  ternary_summary_X_ele$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
ternary_summary_X_ele <- ternary_summary_X_ele %>% full_join(ternary_summary_X_ele_confint, by="est") %>% unique()

ternary_summary_XY_and <-
  ternary_summary %>%
  filter(card_type=="XY", guess_type=="XandY")
ternary_summary_XY_and_confint <-
  ternary_summary_XY_and$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
ternary_summary_XY_and <- ternary_summary_XY_and %>% full_join(ternary_summary_XY_and_confint, by="est") %>% unique()

ternary_summary_X_and <-
  ternary_summary %>%
  filter(card_type=="X", guess_type=="XandY")
ternary_summary_X_and_confint <-
  ternary_summary_X_and$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
ternary_summary_X_and <- ternary_summary_X_and %>% full_join(ternary_summary_X_and_confint, by="est") %>% unique()

ternary_summary_XY_or <-
  ternary_summary %>%
  filter(card_type=="XY", guess_type=="XorY")
ternary_summary_XY_or_confint <-
  ternary_summary_XY_or$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
ternary_summary_XY_or <- ternary_summary_XY_or %>% full_join(ternary_summary_XY_or_confint, by="est") %>% unique()

ternary_summary_X_or <-
  ternary_summary %>%
  filter(card_type=="X", guess_type=="XorY")
ternary_summary_X_or_confint <-
  ternary_summary_X_or$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
ternary_summary_X_or <- ternary_summary_X_or %>% full_join(ternary_summary_X_or_confint, by="est") %>% unique()

ternary_summary <- bind_rows(ternary_summary_X_cat, ternary_summary_XY_cat, ternary_summary_XY_ele, ternary_summary_X_ele, ternary_summary_XY_and, ternary_summary_X_and, ternary_summary_XY_or, ternary_summary_X_or)

ternary_summary$response <- factor(ternary_summary$response, levels = c("Wrong","Neither", "Right"))
ternary_summary$guess_type <- factor(ternary_summary$guess_type, levels = c("Z","X","XandY","XorY"))
ternary_summary$guess_type <- recode(ternary_summary$guess_type, Z = "elephant", X = "cat", XandY = "cat and dog", XorY ="cat or dog")
ternary_summary <- ternary_summary %>% rename(proportion = "est")
```

```{r ternaryPlot, fig.height=3.5, fig.cap = "Proportion responses for the three-alternative (ternary) forced choice judgments in the guessing game."}
ternary_plot<-
  ternary_summary %>%
  ggplot(aes(x=response, y=proportion, fill=response)) +
  geom_bar(stat = "identity", position="dodge", width = 0.6) +
  facet_grid(card_type~guess_type) +
  labs(x="Response Options in the Ternary Task", y="Proportions")+
  theme_few() +
  theme(text = element_text(size=12), axis.text.x = element_text(angle = 90, hjust = 1)) +
    scale_fill_manual(values = c("red4","grey", "springgreen3"), guide=FALSE) +
  geom_linerange(aes(ymin=lwr.ci,ymax=upr.ci))

cat <- 
  readJPEG("experiments/figs/cat_card.jpg") %>%
  rasterGrob(width=0.9)

cat_dog <- 
  readJPEG("experiments/figs/catdog_card.jpg")%>%
  rasterGrob(width=0.9)

ternary_plot_g <- ggplot_gtable(ggplot_build(ternary_plot))

strips <- grep("strip-r", ternary_plot_g$layout$name)

new_grobs <- list(cat, cat_dog)

ternary_plot_g <- with(ternary_plot_g$layout[strips,],
          gtable_add_grob(ternary_plot_g, new_grobs,
                          t=t, l=l, b=b, r=r, name="cards"))        
ternary_plot_g$widths[[11]] <- unit(2.5,"cm")

grid.draw(ternary_plot_g)

ternaryExh_wrong <- ternary_summary %>% filter(card_type=="XY", guess_type=="cat", response=="Wrong")
ternaryScalar_wrong <- ternary_summary %>% filter(card_type=="XY", guess_type=="cat or dog", response=="Wrong")
ternaryExh_neither <- ternary_summary %>% filter(card_type=="XY", guess_type=="cat", response=="Neither")
ternaryScalar_neither <- ternary_summary %>% filter(card_type=="XY", guess_type=="cat or dog", response=="Neither")
```

Figure \@ref(fig:ternaryPlot) shows the proportion of "right", "neither", and "wrong" responses in the ternary task. Similar to the binary task, participants considered a guess wrong when the mentioned animal was not on the card, and "right" when the mentioned animal was the only animal on the card. However, in exhaustive trials when the fictional character only guessed one of the two animals on the card, participants considered the guess "wrong" `r round(ternaryExh_wrong$proportion*100)`% of the time and neither wrong nor right `r round(ternaryExh_neither$proportion*100)`% of the time. If a conjunction of animals was guessed and only one animal was present on the card, participants considered the guess "wrong". As expected, when a conjunction was used and both animals were present, participants considered the guess "right". Similarly, participants considerd the guess "right" when a disjunction was used and only one of the animals was on the card. However, in scalar trials that both animals were on the card and a disjunction was gueassed, participants judged the guess "wrong" `r round(ternaryScalar_wrong$proportion*100)`% of the time and "neither" `r round(ternaryScalar_neither$proportion*100)`% of the time.

```{r quaternaryData}
quaternary_summary<-
  data %>%
  filter(response_type=="quaternary") %>%
  group_by(card_type, guess_type, response) %>%
  summarize(count=n()) %>%
  group_by(card_type, guess_type) %>%
  mutate(total = sum(count), est=count/total)

quaternary_summary_X_cat <-
  quaternary_summary %>%
  filter(card_type=="X", guess_type=="X")
quaternary_summary_X_cat_confint <-
  quaternary_summary_X_cat$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
quaternary_summary_X_cat <- quaternary_summary_X_cat %>% full_join(quaternary_summary_X_cat_confint, by="est")

quaternary_summary_XY_cat <-
  quaternary_summary %>%
  filter(card_type=="XY", guess_type=="X")
quaternary_summary_XY_cat_confint <-
  quaternary_summary_XY_cat$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
quaternary_summary_XY_cat <- quaternary_summary_XY_cat %>% full_join(quaternary_summary_XY_cat_confint, by="est")

quaternary_summary_XY_ele <-
  quaternary_summary %>%
  filter(card_type=="XY", guess_type=="Z")
quaternary_summary_XY_ele_confint <-
  quaternary_summary_XY_ele$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
quaternary_summary_XY_ele <- quaternary_summary_XY_ele %>% full_join(quaternary_summary_XY_ele_confint, by="est") %>% unique()

quaternary_summary_X_ele <-
  quaternary_summary %>%
  filter(card_type=="X", guess_type=="Z")
quaternary_summary_X_ele_confint <-
  quaternary_summary_X_ele$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
quaternary_summary_X_ele <- quaternary_summary_X_ele %>% full_join(quaternary_summary_X_ele_confint, by="est") %>% unique()

quaternary_summary_XY_and <-
  quaternary_summary %>%
  filter(card_type=="XY", guess_type=="XandY")
quaternary_summary_XY_and_confint <-
  quaternary_summary_XY_and$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
quaternary_summary_XY_and <- quaternary_summary_XY_and %>% full_join(quaternary_summary_XY_and_confint, by="est") %>% unique()

quaternary_summary_X_and <-
  quaternary_summary %>%
  filter(card_type=="X", guess_type=="XandY")
quaternary_summary_X_and_confint <-
  quaternary_summary_X_and$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
quaternary_summary_X_and <- quaternary_summary_X_and %>% full_join(quaternary_summary_X_and_confint, by="est") %>% unique()

quaternary_summary_XY_or <-
  quaternary_summary %>%
  filter(card_type=="XY", guess_type=="XorY")
quaternary_summary_XY_or_confint <-
  quaternary_summary_XY_or$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
quaternary_summary_XY_or <- quaternary_summary_XY_or %>% full_join(quaternary_summary_XY_or_confint, by="est") %>% unique()

quaternary_summary_X_or <-
  quaternary_summary %>%
  filter(card_type=="X", guess_type=="XorY")
quaternary_summary_X_or_confint <-
  quaternary_summary_X_or$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
quaternary_summary_X_or <- quaternary_summary_X_or %>% full_join(quaternary_summary_X_or_confint, by="est") %>% unique()

quaternary_summary <- bind_rows(quaternary_summary_X_cat, quaternary_summary_XY_cat, quaternary_summary_XY_ele, quaternary_summary_X_ele, quaternary_summary_XY_and, quaternary_summary_X_and, quaternary_summary_XY_or, quaternary_summary_X_or)

quaternary_summary$response <- factor(quaternary_summary$response, levels = c("Wrong","Kinda Wrong", "Kinda Right", "Right"))
quaternary_summary$guess_type <- factor(quaternary_summary$guess_type, levels = c("Z","X","XandY","XorY"))
quaternary_summary$guess_type <- recode(quaternary_summary$guess_type, Z = "elephant", X = "cat", XandY = "cat and dog", XorY ="cat or dog")
quaternary_summary <- quaternary_summary %>% rename(proportion = "est")
```

```{r quaternaryPlot, fig.height=4, fig.cap = "Proportion responses for the four-alternative (quatenary) forced choice judgments in the guessing game."}
quaternary_plot<-
  quaternary_summary %>%
  ggplot(aes(x=response, y=proportion, fill=response)) +
  geom_bar(stat = "identity", position="dodge", width = 0.6) +
#  geom_linerange(aes(ymax=cih, ymin=cil), position= position_dodge(width=0.9)) + 
  facet_grid(card_type~guess_type) +
  labs(x="Response Options for the Quaternary Task", y="Proportions")+
  theme_few() +
  theme(text = element_text(size=12), axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_fill_manual(values = c("red4", "red3", "springgreen2", "springgreen3"), guide=FALSE) + 
  geom_linerange(aes(ymin=lwr.ci,ymax=upr.ci))

cat <- 
  readJPEG("experiments/figs/cat_card.jpg") %>%
  rasterGrob(width=0.9)

cat_dog <- 
  readJPEG("experiments/figs/catdog_card.jpg")%>%
  rasterGrob(width=0.9)

quaternary_plot_g <- ggplot_gtable(ggplot_build(quaternary_plot))

strips <- grep("strip-r", quaternary_plot_g$layout$name)

new_grobs <- list(cat, cat_dog)

quaternary_plot_g <- with(quaternary_plot_g$layout[strips,],
          gtable_add_grob(quaternary_plot_g, new_grobs,
                          t=t, l=l, b=b, r=r, name="cards"))        
quaternary_plot_g$widths[[11]] <- unit(2.5,"cm")

grid.draw(quaternary_plot_g)

quaternaryExh_kindaRight <- quaternary_summary %>% filter(card_type=="XY", guess_type=="cat", response=="Kinda Right")
quaternaryExh_wrong <- quaternary_summary %>% filter(card_type=="XY", guess_type=="cat", response=="Wrong")
quaternaryExh_kindaWrong <- quaternary_summary %>% filter(card_type=="XY", guess_type=="cat", response=="Kinda Wrong")

quaternaryScalar_wrong <- quaternary_summary %>% filter(card_type=="XY", guess_type=="cat or dog", response=="Wrong")
quaternaryScalar_kindaWrong <- quaternary_summary %>% filter(card_type=="XY", guess_type=="cat or dog", response=="Kinda Wrong")
quaternaryScalar_kindaRight <- quaternary_summary %>% filter(card_type=="XY", guess_type=="cat or dog", response=="Kinda Right")
```

Figure \@ref(fig:quaternaryPlot) shows the proportion of "right", "kinda right", "kinda wrong", and "wrong" responses in the quaternary task. Similar to the results seen previously, the control trials turned out as expected. Participants considered a guess "wrong" if the animal guessed was not on the card and "right" if it was the only animal on the card. If a conjunctin of animals was guessed and both animals were on the card the guess was "right". However, when only one of the animals on the card was guessed (exhaustive trials), participants judged the guess "wrong" `r round(quaternaryExh_wrong$proportion*100)`% of the time, "kinda wrong" `r round(quaternaryExh_kindaWrong$proportion*100)`% of the time, and "kinda right" `r round(quaternaryExh_kindaRight$proportion*100)`% of the times. Perhaps surprisingly, when a conjunction was used and only one of the animals was on the card, participants considered the guess "wrong" most of the time, but they often considered it "kinda wrong" or even "kinda right". This suggests that perhaps participants considered a notion of partially true or correct statement in our experimental setting. Disjunctive guesses with one or two animals on the card showed similar response patterns with participants choosing the "kinda right" and "right" options most of the time. When both animals were on the card with a disjunctive guess (scalar trials), participants judged the guess "wrong" `r round(quaternaryScalar_wrong$proportion*100)`% of the time, "kinda wrong" `r round(quaternaryScalar_kindaWrong$proportion*100)`% of the time, and "kinda right" `r round(quaternaryScalar_kindaRight$proportion*100)`% of the times.

```{r quinaryData}
quinary_summary<-
  data %>%
  filter(response_type=="quinary") %>%
  group_by(card_type, guess_type, response) %>%
  summarize(count=n()) %>%
  group_by(card_type, guess_type) %>%
  mutate(total = sum(count), est=count/total)

quinary_summary_X_cat <-
  quinary_summary %>%
  filter(card_type=="X", guess_type=="X")
quinary_summary_X_cat_confint <-
  quinary_summary_X_cat$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
quinary_summary_X_cat <- quinary_summary_X_cat %>% full_join(quinary_summary_X_cat_confint, by="est")

quinary_summary_XY_cat <-
  quinary_summary %>%
  filter(card_type=="XY", guess_type=="X")
quinary_summary_XY_cat_confint <-
  quinary_summary_XY_cat$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
quinary_summary_XY_cat <- quinary_summary_XY_cat %>% full_join(quinary_summary_XY_cat_confint, by="est")

quinary_summary_XY_ele <-
  quinary_summary %>%
  filter(card_type=="XY", guess_type=="Z")
quinary_summary_XY_ele_confint <-
  quinary_summary_XY_ele$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
quinary_summary_XY_ele <- quinary_summary_XY_ele %>% full_join(quinary_summary_XY_ele_confint, by="est") %>% unique()

quinary_summary_X_ele <-
  quinary_summary %>%
  filter(card_type=="X", guess_type=="Z")
quinary_summary_X_ele_confint <-
  quinary_summary_X_ele$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
quinary_summary_X_ele <- quinary_summary_X_ele %>% full_join(quinary_summary_X_ele_confint, by="est") %>% unique()

quinary_summary_XY_and <-
  quinary_summary %>%
  filter(card_type=="XY", guess_type=="XandY")
quinary_summary_XY_and_confint <-
  quinary_summary_XY_and$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
quinary_summary_XY_and <- quinary_summary_XY_and %>% full_join(quinary_summary_XY_and_confint, by="est") %>% unique()

quinary_summary_X_and <-
  quinary_summary %>%
  filter(card_type=="X", guess_type=="XandY")
quinary_summary_X_and_confint <-
  quinary_summary_X_and$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
quinary_summary_X_and <- quinary_summary_X_and %>% full_join(quinary_summary_X_and_confint, by="est") %>% unique()

quinary_summary_XY_or <-
  quinary_summary %>%
  filter(card_type=="XY", guess_type=="XorY")
quinary_summary_XY_or_confint <-
  quinary_summary_XY_or$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
quinary_summary_XY_or <- quinary_summary_XY_or %>% full_join(quinary_summary_XY_or_confint, by="est") %>% unique()

quinary_summary_X_or <-
  quinary_summary %>%
  filter(card_type=="X", guess_type=="XorY")
quinary_summary_X_or_confint <-
  quinary_summary_X_or$count %>%
  MultinomCI(conf.level = 0.95, method = "sisonglaz") %>% data.frame()
quinary_summary_X_or <- quinary_summary_X_or %>% full_join(quinary_summary_X_or_confint, by="est") %>% unique()

quinary_summary <- bind_rows(quinary_summary_X_cat, quinary_summary_XY_cat, quinary_summary_XY_ele, quinary_summary_X_ele, quinary_summary_XY_and, quinary_summary_X_and, quinary_summary_XY_or, quinary_summary_X_or)

quinary_summary$response <- factor(quinary_summary$response, levels = c("Wrong","Kinda Wrong", "Neither", "Kinda Right", "Right"))
quinary_summary$guess_type <- factor(quinary_summary$guess_type, levels = c("Z","X","XandY","XorY"))
quinary_summary$guess_type <- recode(quinary_summary$guess_type, Z = "elephant", X = "cat", XandY = "cat and dog", XorY ="cat or dog")
quinary_summary <- quinary_summary %>% rename(proportion = "est")
```

```{r quinaryPlot, fig.height=4, fig.cap = "Proportion responses for the five-alternative (quinary) forced choice judgments in the guessing game."}
quinary_plot<-
  quinary_summary %>%
  ggplot(aes(x=response, y=proportion, fill=response)) +
  geom_bar(stat = "identity", position="dodge", width = 0.6) +
#  geom_linerange(aes(ymax=cih, ymin=cil), position= position_dodge(width=0.9)) + 
  facet_grid(card_type~guess_type) +
  labs(x="Response Options in the Quinary Task", y="Proportions")+
  theme_few() +
  theme(text = element_text(size=12), axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_fill_manual(values = c("red4", "red3", "grey", "springgreen2", "springgreen3"), guide=FALSE) +
  geom_linerange(aes(ymin=lwr.ci,ymax=upr.ci))

cat <- 
  readJPEG("experiments/figs/cat_card.jpg") %>%
  rasterGrob(width=0.9)

cat_dog <- 
  readJPEG("experiments/figs/catdog_card.jpg")%>%
  rasterGrob(width=0.9)

quinary_plot_g <- ggplot_gtable(ggplot_build(quinary_plot))

strips <- grep("strip-r", quinary_plot_g$layout$name)

new_grobs <- list(cat, cat_dog)

quinary_plot_g <- with(quinary_plot_g$layout[strips,],
          gtable_add_grob(quinary_plot_g, new_grobs,
                          t=t, l=l, b=b, r=r, name="cards"))        
quinary_plot_g$widths[[11]] <- unit(2.5,"cm")

grid.draw(quinary_plot_g)

quinaryExh_kindaRight <- quinary_summary %>% filter(card_type=="XY", guess_type=="cat", response=="Kinda Right")
quinaryExh_wrong <- quinary_summary %>% filter(card_type=="XY", guess_type=="cat", response=="Wrong")
quinaryExh_kindaWrong <- quinary_summary %>% filter(card_type=="XY", guess_type=="cat", response=="Kinda Wrong")
quinaryExh_Neither <- quinary_summary %>% filter(card_type=="XY", guess_type=="cat", response=="Neither")

quinaryyScalar_wrong <- quinary_summary %>% filter(card_type=="XY", guess_type=="cat or dog", response=="Wrong")
quinaryScalar_kindaWrong <- quinary_summary %>% filter(card_type=="XY", guess_type=="cat or dog", response=="Kinda Wrong")
quinaryScalar_kindaRight <- quinary_summary %>% filter(card_type=="XY", guess_type=="cat or dog", response=="Kinda Right")
quinaryScalar_Neither <- quinary_summary %>% filter(card_type=="XY", guess_type=="cat or dog", response=="Neither")
```

Finally, Figure \@ref(fig:quinaryPlot) shows the proportion of "right", "kinda right", "neither", "kinda wrong", and "wrong" responses in the quinary task. Since the results for the control trials were identical to previous tasks, we do not repeat them here. In exhaustive trials where two animals were on the card and only one of them was guessed, participants chose "kinda right" the majority of times (`r round(quinaryExh_kindaRight$proportion*100)`%). Again perhaps surprisingly, when only one animal was on the card and the guess was a conjunction, responses were equally split among "wrong", "kinda wrong", and "kinda right" responses. With disjunctive guesses, partitipants were more likely to choose "right" and "kinda right" options. When only one animal was on the card, participants considered the disjunctive guess as "right" more often. When both animals were on the card (scalar trials), participants judged the disjunctive guess as "kinda right" `r round(quinaryScalar_kindaRight$proportion*100)`% of the time.

<!--
In four trial types, participants provided the same responses regardless of how many response options were made available. The first two trial types include the guesses with an animal not on the card at all. In such trials, whether there was one or two animals on the card, participants considered the guess wrong. In the third and the fourth trial types, the gueses matched the aimal(s) on the card; if there was one animal on the card, that animal was mentioned and if there were two, both were mentioned using a conjunction. The fact that participants chose the extreme ends of the scale for these trial types and the responses did not change when intermediate options were provided suggests that participant judgments truly aligned with the opposite ends of the scale in these trial types. 

In the trial type where the guess was a conjunction but only one of the animals was on the card, participants considered the guess wrong when fewer options were provided. However, in tasks with more options (four and five), participant responses were distributed between "wrong", "kinda wrong", and "kinda right" options, with the majority of responses being "wrong" or "kinda wrong". The graded nature of the judgments in this trial type suggests that participants do not consider such guesses as completely wrong. Similarly, 

Finally, in exhaustive and scalar implicature trial types participants were less likely to choose the option "wrong" when they were given more intermediate options. Even though in the binary task task recorded participant judgments as  

semantic violations vs. pragmatic violations
-->

## Analysis

```{r implicatureResultss}
implicature_results<-
  implicature_rate %>%
  group_by(response_type, trial_type, implicature, definition) %>%
  summarize(counts = n()) %>%
  group_by(response_type, trial_type, definition) %>%
  mutate(total = sum(counts), proportion = counts/total)

implicature_plot <- 
  implicature_results %>% filter(implicature==1)

binomial_confint<- binom.confint(implicature_plot$counts, implicature_plot$total, method="logit") %>%
  rename(counts = "x", total="n")

implicature_results <- implicature_plot %>% full_join(binomial_confint, by=c("total", "counts")) %>% unique

implicature_results$response_type <- fct_relevel(implicature_results$response_type, "binary", "ternary", "quaternary", "quinary")
implicature_results$implicature <- as.factor(implicature_results$implicature)
```

Our primary goal in this study was to check whether the estimated "implicature rate" in the experimental task is affected by the linking assumption and the number of response options available in the task. Our analysis in this section focuses on these three elements. As mentioned before, two trial types were predicted to include pragmatic implicatures. First, trials where two animals were on the card but the fictional character guessed with a disjunction (scalar); for example "cat or dog" when the card has both a cat and a dog on it. Second, trials where there were two animals on the card but the character guessed only one (exhaustive); for example "cat" when the card had a cat and a dog on it. We called such trials "exhaustive". In our assessment of implicature rate, we focus on these two trial types.

We considered two linking assumptions. First we defined a weak (lenient) linking assumption in which any response lower than the maximum point on the scale (i.e. "right") is considered evidence for implicature computation. Second, we defined a strong (strict) linking assumption that only considered the lowest point on the scale (i.e. "wrong") as evidence for implicature computation. For each condition in our study (binary, ternary, quaternary, and quinary) and each implicature trial type (exhaustive and scalar), we computed a weak and a strong implicature rate. Figure \@ref(fig:implicatureRatePlot) shows these computed rates.

Comparing the strong and weak rows on Figure \@ref(fig:implicatureRatePlot), we see that a weak linking assumption tends to estimate higher implicature rates, especially in tasks with more response options. With a strong linking assumption, the binary and possibly ternary judgment tasks derive higher implicature rates than quaternary and quinary tasks. With a weak linking assumption, the pattern is reversed. Quaternary and quinary tasks estimate higher rates than binary and ternary tasks. The patterns show that estimates of "implicature rate" depend on linking assumptions and the number of responses available to participants in the study.

Comparing the exhaustive and scalar columns of Figure \@ref(fig:implicatureRatePlot), we see that with a strong linking assumption, there are slightly higher rates for scalar implicatures in the binary and ternary tasks. With a weak linking assumption, there may be slightly higher rates for scalar implicatures in the binary and ternary while the rates may be lower in the quaternary and quinary tasks. In what follows, we formally test the effect of linking assumption and response options on exhaustive and scalar implicature rates.

```{r implicatureRatePlot, fig.width=5, fig.height=3, fig.cap = "Implicature rate in exhaustive and scalar trials of the binary, ternary, quaternary, and quinary versions of the guessing game, computed once with a strong linking assumption and once with a weak linking assumption."}
ggplot(implicature_results, aes(x=response_type,y=proportion)) +
  geom_bar(aes(fill=definition), stat="identity") +
  facet_grid(trial_type~definition, drop = TRUE) +
  theme_few() + 
  labs(x="Number of Response Options in the Task", y="Inferred Implicature Rate") +
  theme(text = element_text(size=12), axis.text.x = element_text(angle = 45, hjust = 1)) +
  geom_linerange(aes(ymin=lower,ymax=upper)) + guides(fill=FALSE)
```

```{r implicatureRate, warning=FALSE}
# The analysis takes time to run. To reproduce please run the commented lines. We have saved the results we obtained as a dataframe.

#library(brms)

#implicature_analysis_b <- brm(implicature ~ definition * response_type * trial_type + (definition*trial_type*response_type|card) + (definition*trial_type|participant), family="bernoulli", data=implicature_rate, control = list(adapt_delta = 0.99, max_treedepth = 15))

#saveRDS(implicature_analysis_b, file = "implicature_analysis_b")
#implicature_analysis_b <- readRDS("implicature_analysis_b")

#fixed_effects <- data.frame(fixef(implicature_analysis_b))
#write.csv(fixed_effects, "experiments/main/fixed_effect_estimates.csv")

fixed_effects <- read_csv("experiments/main/fixed_effect_estimates.csv")

fixed_effects$X1 <- c("Intercept", "Link = Weak", "Task = Quaternary", "Task = Quinary", "Task = Ternary", "Implicature = Scalar", "Link = Weak : Task = Quaternary", "Link = Weak : Task = Quinary", "Link = Weak : Task = Ternary", "Link = Weak : Implicature = Scalar", "Task = Quaternary : Implicature = Scalar", "Task = Quinary : Implicature = Scalar", "Task = Ternary : Implicature = Scalar", "Link=Weak : Task=Quaternary : Implicature=Scalar", "Link=Weak : Task=Quinary : Implicature=Scalar", "Link=Weak : Task=Ternary : Implicature=Scalar")

fixed_effects_table <- 
  fixed_effects %>%
  rename(Predictors = "X1", `2.5%` = "X2.5.ile", `97.5%` = "X97.5.ile") %>%
  select(-Est.Error)

kable(fixed_effects_table,digits = 2 , caption = "\\label{tab:modeltable}Model Parameter Estimates and Their Credible Intervals", booktabs = T)
```

For our formal analysis, we used a bayesian binomial mixed effects model using the R packge "brms" [@burkner2016brms]. The model had the fixed effects of response type (binary, ternary, quaternary, quinary), linking assumption (strong vs. weak), and trial type (exhaustive vs. scalar), as well as their two way and three way interactions. We also included random intercepts and slopes for items (cards) and participants. Following @barr2013random, we used the maximal random effects structure by including random slopes for all our fixed effects and their interactions. Since the number of response options was a between participant variable we did not include random slopes of response options for participants. Four chains converged after 2000 iterations each (warmup = 1000). Table \@ref(tab:modeltable) summarizes the mean parameter estimates and their 95% credible intervals. $\hat{R}=1$ for all estimated parameters. All the analytical decisions described here were pre-registered[^1].

The model provided evidence for two effects in the study. First, there was a main effect of trials such that scalar trials had slightly higher implicature rates than exhaustive trials (Mean Estimate = `r round(fixed_effects$Estimate[6],2)`, 95% Credible Interval=[`r round(fixed_effects$X2.5.ile[6],2)`, `r round(fixed_effects$X97.5.ile[6],2)`]). Second, there was an interaction between linking assumption and number of resopnse options such that the quaternary task (Mean Estimate = `r round(fixed_effects$Estimate[7],2)`, 95% Credible Interval=[`r round(fixed_effects$X2.5.ile[7],2)`, `r round(fixed_effects$X97.5.ile[7],2)`]) and the quinary task  (Mean Estimate = `r round(fixed_effects$Estimate[8],2)`, 95% Credible Interval=[`r round(fixed_effects$X2.5.ile[8],2)`, `r round(fixed_effects$X97.5.ile[8],2)`]) with a weak linking assumption had higher implicature rates. 

[^1]: You can access our pre-registration at [https://aspredicted.org/tq3sz.pdf](https://aspredicted.org/tq3sz.pdf)

## Discussion

We asked whether the linking assumptions and the number of response options available to participants affects the inferred implicature rate in an experimental study. The results presented here suggest they do. A linking assupmtion that considered the highest point on the scale as literal and any lower point as pragmatic (weak link) reported higher implicature rates in tasks with 4 or 5 options. A linking assumption that considered the lowest point on the scale as pragmatic and any higher point as literal (strong link) reported lower implicature rates in tasks with 4 or 5 options. The results suggest that the choice of linking assumption is a crucial analytical step that can significantly impact the conclusions of truth value judgment tasks with more than two response options. The lower rate of implicatures with a strong linking assumption implies that in such studies, strong linking assumptions may underestimate participants' pragmatic competence.

While the binary truth value judgement task avoids the analytic decision between strong and weak linking assumptions, our results suggest that binary tasks can also underestimate participants' pragmatic competence. In binary tasks, participants are often given the lowest and highest points on a scale ("wrong" vs. "right") and are asked to report pragmatic infelicities using the lowest point (e.g. "wrong"). Our study showed that in trials with true but infelicitous descriptions (implicature trials), participants often avoided the lowest point on the scale if they were given more intermediate options. Even though the option "wrong" was available to participants in all tasks, participants in tasks with intermediate options chose it less often. In computing implicature rate, this pattern manifested itself as a decrease in implicature rate with strong link when more response options were provided, and increase in implicature rate with weak link when more response options were provided. These conclusions are in line with @Katsos2011's argument that pragmatic violations are not as severe as semantic violations and participants do not penalize them as much. Providing participants with only the extreme opposits of the scale (e.g. wrong/right, false/true) when pragmatic violations are considered to be of an intermediate nature risks misrepresentation of participants' pragmatic competence.

This study did not investigate the effect of option labels on the inferred implicature rate. However, the results provided suggestive evidence that some options better capture participant intuitions of pragmatic infelicities than others. Among the intermediate options, "kinda right" was chosen most often to report pragmatic infelicities. The option "neither" was rarely used in the ternary and quinary tasks (where it was used as a midpoint), suggesting that participants found pragmatic infelicities as different degrees of being "right" and not "neither right nor wrong." Therefore, options that capture grades of being "right" like "kinda right" proved be most suitable for capturing true but infelicitous utterances.

This study had three design features that we would like to investigate in future work. First, the utterances were by a blindfolded character because we wanted to control for violation of ignorance expectations with disjunction. A disjunction such as "A or B" often carries an implication or expectation that the speaker is not certain which alternative actually holds. In future work, we would like to see how the violation of the ignorance expectation would affect the inferred implicature rate by having the fictional character describe the cards while looking at them. Second, in this study we considered exhaustive and scalar implicatures with *or*. We would like to see if similar effects hold for the scalar implicatures with *some*. Finally, our experiment was designed as a guessing game and the exact goal of the game was left implicit. We expect that different goals, for example help the character win more points vs. help the character be more accurate, would affect how strict or lenient participants are with their judgments and ultimately affect the implicature rate in the task. In future work we would like to systematically vary the goal of the game and explore its effects on the inferred implicature rate.

# General Discussion

On the traditional view of the link between implicature and behavior in sentence verification tasks, scalar implicature is conceptualized as a binary, categorical affair - that is, an implicature is either ‘calculated’ or it isn’t, and the behavioral reflexes of this categorical interpretation process should be straightforwardly observed in experimental paradigms. This assumption has concerning implications for how we must approach analysis of variation in behavior on a truth value judgment task; for example, why did the majority of respondents in the binary condition of our experiment answer “Right” to an utterance of cat or dog when the card had both a cat and a dog on it?

To explain the data on the traditional view, are forced to say that a) not all participants calculated the implicature; or that b) some participants who calculated the implicature did not choose the anticipated response (i.e. “Wrong”) due to some other cognitive reflex which ‘overrode’ the implicature; or some mixture of (b) and (c). We might similarly posit that one or both of these factors underlie the variation in the ternary, quatenary, and quinary conditions (e.g. why were participants roughly split between “Right” and “Kind of right” when the utterance was cat or dog and the card had a cat and a dog?). However, the best we can hope for on this approach is an analysis which traces the general qualitative patterns in the data.

We contrast the above view of implicature and its behavioral reflexes with an alternative linking hypothesis which assumes that participants' behavior can be represented using the model of a soft-optimal pragmatic speaker in the RSA framework. This alternative linking hypothesis contrasts with the traditional view in it is rooted in a quantitative formalization of pragmatic competence which provides us a continuous measure of pragmatic reasoning. Recall that in RSA, pragmatically competent listeners are modeled as a continuous probabilistic distribution of possible meanings given an utterance which that listener hears. The probability with which this listener $L_1$ ascribes a meaning s to an utterance u depends upon a prior probability distribution of potential states of the world $P_w$, and upon reasoning about the communicative behavior of a speaker $S_1$. $S_1$ in turn is modeled as a continuous probabilistic distribution of possible utterances given an intended state of affairs the speaker intends to communicate. This distribution is sensitive to a rationality parameter $\alpha$, the production cost $C$ of potential utterances, and a representation of a literal listener $L_0$ whose interpretation of an utterance is in turn a function of that utterance's truth conditional content $[[u]](s)$ and her prior beliefs about the state of the world $P_w(s)$. 
    
    $P_{L_1}(s | u) \propto P_{S_1}(u | s) * P_w(s)$

    $P_{S_1}(u | s) \propto exp(\alpha(log(L_0(s | u)) - C(u))) $
    
    $P_{L_0}(s | u) \propto [[u]](s) * P_w(s)$

In this framework, individuals never categorically draw (or fail to draw) pragmatic inferences about the utterances they hear. For example, exclusivity readings of disjunction or are represented in RSA as relatively low conditional probability of a conjunctive meaning on the $P_L$ distribution, given an utterance of or. Thus, it is not even possible to talk about 'rate' of implicature calculation in the RSA framework. The upshot, as we show below, is that this view of pragmatic competence does allow us to talk explicitly and quantitatively about rates of observed behavior in sentence verification tasks.

First, following Degen & Goodman (2014), we proceed on the assumption that behavior on sentence verification tasks, such as truth value judgment tasks, is best modeled as a function of an individual’s mental representation of a cooperative interlocutor ($S_1$ in the language of RSA) rather than of a pragmatic listener who interprets utterances ($P_{L_1}$). In their paper, Degen & Goodman argue that sentence verification tasks are relatively more sensitive to contextual manipulations (such as manipulation of the Question Under Discussion) than are sentence interpretation tasks, and that this follows if sentence interpretation tasks - but not sentence verification tasks - require an additional layer of counterfactual reasoning about the intentions of a cooperative speaker. 

A main desideratum of a behavioral linking hypothesis given the RSA view of pragmatic competence is to transform continuous probability distributions into categorical outputs (e.g. responses of “Right”/”Wrong” in the case of the binary condition of our experiment). For a given utterance u and an intended communicated meaning w, $S_1$(u | w) outputs a conditional probability of u given w. For example, in the binary condition of our experiment where a participant evaluated cat or dog when there were both animals on the card, the participant has access to the mental representation of $S_1$ and hence to the $S_1$ conditional probability of hearing the utterance cat or dog given a dog and cat card: $S_1$(cat or dog | cat and dog). According to the linking hypothesis advanced here, the participant provides a particular response to u if the RSA speaker probability of u lies within a particular probability interval, given an observed state of the world (i.e. the configuration of animals on the card in our experiment). We model a responder, R, who in the binary condition responds “Right” to an utterance u in world w just in case $S_1$(u | w) exceeds some probability threshold $\theta$:

R(u, w, $\theta$) 

= “Right” iff $S_1$(u | w) $>$ $\theta$

= “Wrong” otherwise

In the experiment conditions where there are more than two choices, we model the range of possible behavioral responses for R with the introduction of intermediate probability thresholds. For example, in the ternary condition, R(u, w, $\theta_1$ , $\theta_2$) is “Right” iff $S_1$(u | w) > $\theta_1$ and “Neither” iff $\theta_1$ > $S_1$(u | w) > $\theta_2$. To fully generalize the model to our five experimental conditions, we say that R takes as its input an utterance u, a world state w, and a number of threshold variables dependent on a variable c, corresponding to the experimental condition in which the participant finds herself (e.g. the range of possible responses available to R). 

Given c = “ternary”

R(u, w, $\theta_1$ , $\theta_2$)

= “Right” iff $S_1$(u | w) $>$ $\theta_1$ 

= “Neither” iff $\theta_1$ $>$ $S_1$(u | w) $>$ $\theta_2$ 

= “Wrong” otherwise

Given c = “quatenary”

R(u, w, $\theta_1$ , $\theta_2$, $\theta_3$)

= “Right” iff $S_1$(u | w) $>$ $\theta_1$ 

= “Kinda Right” iff $\theta_1$ $>$ $S_1$(u | w) $>$ $\theta_2$ 

= “Kinda Wrong” iff $\theta_2$ $>$ $S_1$(u | w) $>$ $\theta_3$ 

= “Wrong” otherwise

Given c = “quinary”

R(u, w, $\theta_1$ , $\theta_2$, $\theta_3$. $\theta_4$)

= “Right” iff $S_1$(u | w) $>$ $\theta_1$ 

=“Kinda Right” iff $\theta_1$ $>$ $S_1$(u | w) $>$ $\theta_2$ 

= “Neither” iff $\theta_2$ $>$ $S_1$(u | w) $>$ $\theta_3$ 

= “Kinda Wrong” iff $\theta_3$ $>$ $S_1$(u | w) $>$ $\theta_4$ 

= “Wrong” otherwise 

Bayesian statistical methods provide us a means for estimating the values of these probability thresholds in our RSA model. The basis for the model is a set of possible states of the world, given a universe of three animals - X, Y, and Z - that each may be on some card. We next define a set of possible sentences a speaker might utter, assuming the speaker intends to communicate which animals are on the card. We assume a uniform prior probability of different states of the world and a uniform cost function on utterances. We define a literal listener $L_0$, a pragmatic speaker $S_1$, and a responder $R$ according to our definitions above. Lastly, and assuming a uniform prior distribution over possible values of probability thresholds, we use Bayesian inference to recover a posterior distribution of these thresholds in each experimental condition, given the actual observed rate of response in each condition of the experiment. The results of this parameter estimation analysis are shown in the figures below, where the X axis of each figure corresponds to a threshold value between 0 and 1 and the Y axis corresponds to the posterior probability density of possible values of the threshold. 

```{r echo=FALSE, message=FALSE}
bda <- read.csv("models/webppl_models/bda.csv")

binary_graph <- ggplot(filter(bda, Parameter == "binary_theta"), aes(x=value))+
  # facet_wrap(~Parameter, scales = "free") +
  theme_few() +
  geom_density() +
  scale_fill_grey(guide=FALSE) +
  theme(axis.text.x=element_text(angle=20,hjust=1,vjust=1)) +
  ggtitle("Threshold distribution, binary condition:")

ternary_graph <- ggplot(filter(bda, Parameter == c("ternary_theta1", "ternary_theta2")), aes(x=value))+
  facet_wrap(~Parameter, scales = "free") +
  theme_few() +
  geom_density() +
  scale_fill_grey(guide=FALSE) +
  theme(axis.text.x=element_text(angle=20,hjust=1,vjust=1)) +
  ggtitle("Threshold distributions, ternary condition:")

quatenary_graph <- ggplot(filter(bda, Parameter == c("quatenary_theta1", "quatenary_theta2", "quatenary_theta3")), aes(x=value))+
  facet_wrap(~Parameter, scales = "free") +
  theme_few() +
  geom_density() +
  scale_fill_grey(guide=FALSE) +
  theme(axis.text.x=element_text(angle=20,hjust=1,vjust=1)) +
  ggtitle("Threshold distributions, quatenary condition:")

quinary_graph <- ggplot(filter(bda, Parameter == c("quinary_theta1", "quinary_theta2", "quinary_theta3", "quinary_theta4")), aes(x=value))+
  facet_wrap(~Parameter, scales = "free") +
  theme_few() +
  geom_density() +
  scale_fill_grey(guide=FALSE) +
  theme(axis.text.x=element_text(angle=20,hjust=1,vjust=1)) +
  ggtitle("Threshold distributions, quinary condition:")
binary_graph
ternary_graph
quatenary_graph
quinary_graph
```

The above analysis is a proof of concept for the following idea: by relaxing the assumptions of the traditional view of scalar implicature (namely, that scalar implicatures either are or are not calculated, and that behavior on sentence verification tasks directly reflects this binary interpretation process), we can propose quantitative models of the variation in behavior we observe in experimental settings. 

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
